{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PretrainedAE+CNNClassifier-final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_LZPFRgN90KC",
        "6pyz3odTLHm-",
        "0-SmL5fXylVR",
        "AfePYWDFw3-Y"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "297636decf204bad90e5331dd092ef67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8f5fabdf3e3499788323831b28657e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31356d2c18304a42914b8d6ebdbc60c9",
              "IPY_MODEL_c71778faec5e4d9688f5e82aa3a4aa96"
            ]
          }
        },
        "c8f5fabdf3e3499788323831b28657e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31356d2c18304a42914b8d6ebdbc60c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_48cf129b4ac14a2d9cd66391ee2d1557",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0489fbe5a324651b1c71e5e27afeba4"
          }
        },
        "c71778faec5e4d9688f5e82aa3a4aa96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3cfc8c17f7ca43f5bd188ed05f3a40d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:04&lt;00:00, 38195692.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ccfd56f5fa648d0b699851b14895ebd"
          }
        },
        "48cf129b4ac14a2d9cd66391ee2d1557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0489fbe5a324651b1c71e5e27afeba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cfc8c17f7ca43f5bd188ed05f3a40d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ccfd56f5fa648d0b699851b14895ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZPFRgN90KC",
        "colab_type": "text"
      },
      "source": [
        "## Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSj15dPGylVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# Torch and Torchvision\n",
        "import torchvision \n",
        "#from torch.utils import data\n",
        "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms \n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzPw1X-v99oJ",
        "colab_type": "code",
        "outputId": "89b2c92a-40e6-4059-f1b6-c64f04b05442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "!pip install tensorboardX\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "from tensorboardX import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "logs_base_dir = \"Logs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)\n",
        "# debug ref: https://github.com/pytorch/pytorch/issues/22676"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8PhQbYUfY_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "# PyTorch\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torch import optim, cuda\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "import torch.nn as nn\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Data science tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Image manipulations\n",
        "from PIL import Image\n",
        "# Useful for examining network\n",
        "from torchsummary import summary\n",
        "# Timing utility\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "plt.rcParams['font.size'] = 14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mXsZAsESvLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Ju1zoA5Ng0A"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pyz3odTLHm-",
        "colab_type": "text"
      },
      "source": [
        "### Load Data and check Meta info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djX7Le71ylVG",
        "colab_type": "code",
        "outputId": "f8ce8ea1-4843-4979-f6c7-a7f264dc7144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "297636decf204bad90e5331dd092ef67",
            "c8f5fabdf3e3499788323831b28657e8",
            "31356d2c18304a42914b8d6ebdbc60c9",
            "c71778faec5e4d9688f5e82aa3a4aa96",
            "48cf129b4ac14a2d9cd66391ee2d1557",
            "f0489fbe5a324651b1c71e5e27afeba4",
            "3cfc8c17f7ca43f5bd188ed05f3a40d2",
            "8ccfd56f5fa648d0b699851b14895ebd"
          ]
        }
      },
      "source": [
        "transforms_raw = transforms.Compose([transforms.ToTensor()]) #for conversion to tensors\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transforms_raw)\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transforms_raw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "297636decf204bad90e5331dd092ef67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7MqBpJ4J_Yj",
        "colab_type": "code",
        "outputId": "10f240a6-73d5-40a8-d8f7-7c7dc0aed744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get train, test set sizes\n",
        "trainset_size =len(trainset) \n",
        "testset_size = len(testset)\n",
        "print(trainset_size, testset_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxvxv1lWlDzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainset.targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXfjQkL7z1VI",
        "colab_type": "code",
        "outputId": "11af09fd-cc91-4a2e-f69a-1bc88f83934a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# meta data information\n",
        "trainset.class_to_idx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'airplane': 0,\n",
              " 'automobile': 1,\n",
              " 'bird': 2,\n",
              " 'cat': 3,\n",
              " 'deer': 4,\n",
              " 'dog': 5,\n",
              " 'frog': 6,\n",
              " 'horse': 7,\n",
              " 'ship': 8,\n",
              " 'truck': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGcRudHDOrMe",
        "colab_type": "code",
        "outputId": "70f676b4-8b97-42cb-e193-487431a780ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate mean and std of the pixel values in images and nomalize it while transforming\n",
        "x = np.concatenate([np.asarray(\n",
        "    trainset[i][0].reshape(-1,\n",
        "                             trainset[0][0].shape[0],\n",
        "                             trainset[0][0].shape[1],\n",
        "                             trainset[0][0].shape[2])\n",
        "    ) for i in range(len(trainset))])\n",
        "# print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 3, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGlsJZNMO9wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainset.data[0]\n",
        "#trainset[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9ayz-jdQH4",
        "colab_type": "code",
        "outputId": "fc314d0a-c901-4f8d-85af-58fd8fd78323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# calculate the mean and std along the (0, 1) axes\n",
        "train_mean = np.mean(x, axis=(0,2,3))\n",
        "train_std = np.std(x, axis=(0,2,3))\n",
        "print(train_mean)\n",
        "print(train_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.4914009  0.48215896 0.4465308 ]\n",
            "[0.24703279 0.24348423 0.26158753]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-SmL5fXylVR",
        "colab_type": "text"
      },
      "source": [
        "### Filter 50% of the labels for training (bird, deer and truck)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAFazUb3xZcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# traning data labels for bird = 2, deer =4, truck = 9 need to be cut down to half \n",
        "custom_data=[]\n",
        "custom_label =[]\n",
        "bird = 0\n",
        "deer =0\n",
        "truck = 0\n",
        "limit = 2500\n",
        "for i in range(len(trainset)):#range(20):\n",
        "    train_data_i = trainset[i][0]\n",
        "    train_label_i = trainset[i][1]\n",
        "    rules = [train_label_i == 0,\n",
        "             train_label_i == 1,\n",
        "             train_label_i == 3,\n",
        "             train_label_i == 5,\n",
        "             train_label_i == 6,\n",
        "             train_label_i == 7,\n",
        "             train_label_i == 8]\n",
        "    if any(rules):\n",
        "        custom_data.append(train_data_i)\n",
        "        custom_label.append(train_label_i)      \n",
        "    elif train_label_i==2:\n",
        "      if bird<limit:\n",
        "        bird+=1\n",
        "        custom_data.append(train_data_i)\n",
        "        custom_label.append(train_label_i)\n",
        "    elif train_label_i==4:\n",
        "      if deer<limit:\n",
        "        deer+=1\n",
        "        custom_data.append(train_data_i)\n",
        "        custom_label.append(train_label_i)\n",
        "    elif train_label_i==9:\n",
        "      if truck<limit:\n",
        "        truck+=1\n",
        "        custom_data.append(train_data_i)\n",
        "        custom_label.append(train_label_i)        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odwxw6CindbK",
        "colab_type": "code",
        "outputId": "c5061792-b095-4246-f954-f0e9f7f0acc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(custom_data))\n",
        "#tensor_clabel = torch.Tensor(custom_label)\n",
        "#tensor_cdata = torch.Tensor(custom_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykl8lQP3VKwR",
        "colab_type": "code",
        "outputId": "f15fc4dd-28e1-404a-89d5-0f37db06f0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(custom_data))\n",
        "tensor_clabel = custom_label\n",
        "tensor_cdata = custom_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1fz4Enu7ZFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = pd.DataFrame({'img_data':custom_data,'class_label':custom_label})\n",
        "#df.iloc[1]['img_data'].shape\n",
        "#df.to_csv('./data/xx.csv')\n",
        "#ss = pd.read_csv('./data/xx.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkIXG-Ovk6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfePYWDFw3-Y",
        "colab_type": "text"
      },
      "source": [
        "### Build Customed Dataset with Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5I-2ZYLBPXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# my_dataset = TensorDataset(tensor_cdata,tensor_clabel)\n",
        "# ref:https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n",
        "# By default transforms are not supported for TensorDataset.\n",
        "# but we want to use the the horizonalflip etc as data augumentation methods \n",
        "# Threfore we will create our custom class to add that option.\n",
        "# ref: https://stackoverflow.com/questions/55588201/pytorch-transforms-on-tensordataset\n",
        "class CustomTensorDataset():\n",
        "    \"\"\"TensorDataset with support of transforms.\n",
        "    \"\"\"\n",
        "    def __init__(self, images,labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        labels = self.labels[index]\n",
        "        image_tensor = self.images[index]\n",
        "        images = transforms.ToPILImage()(image_tensor)\n",
        "\n",
        "        if self.transform:\n",
        "            images = self.transform(images)\n",
        "\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUrbXHffdpDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomRotation(10),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(train_mean, train_std)\n",
        "                                     ])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(train_mean, train_std)\n",
        "                                    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxdMhhBORFP_",
        "colab_type": "code",
        "outputId": "65bef8df-f7a0-4e0d-90ac-4011a0b29b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we will use a rough 80% train, 10% val, 10% test spilt \n",
        "cifar10datasets = {\n",
        "    'train':CustomTensorDataset(images=tensor_cdata,labels=tensor_clabel,\n",
        "     transform=train_transform)\n",
        "            }\n",
        "valtest_dataset= datasets.CIFAR10(\n",
        "         root='./data', train=False, download=True, \n",
        "         transform=test_transform)\n",
        "cifar10datasets['test'], cifar10datasets['val'] = torch.utils.data.random_split(valtest_dataset, [5000, 5000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1NiG6AV8Ys4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10y_gLacQ6Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpI2G2OJLAY3",
        "colab_type": "text"
      },
      "source": [
        "### Build  Data Iterators\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQRZHhIKdag8",
        "colab_type": "text"
      },
      "source": [
        "To avoid loading all of the data into memory at once, we use training `DataLoaders`. First, we create a dataset object from the image folders, and then we pass these to a `DataLoader`. At training time, the `DataLoader` will load the images from disk, apply the transformations, and yield a batch. To train and validation, we'll iterate through all the batches in the respective `DataLoader`. \n",
        "\n",
        "One crucial aspect is to `shuffle` the data before passing it to the network. This means that the ordering of the image categories changes on each pass through the data (one pass through the data is one training epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im6_z3g_i4Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=16\n",
        "dataloaders = {\n",
        "'train' : DataLoader(cifar10datasets['train'], batch_size=batch_size,shuffle=True, num_workers=2),\n",
        "'test' : DataLoader(cifar10datasets['test'], batch_size=batch_size,shuffle=True, num_workers=2),\n",
        "'val' : DataLoader(cifar10datasets['val'], batch_size=batch_size,shuffle=True, num_workers=2)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXJwOYFb5s54",
        "colab_type": "text"
      },
      "source": [
        "## Load Prtrained Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OdDCsvo51TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ZmQ2aM51WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Input size to the Autoencoder : [batch_size, 3, 32, 32]\n",
        "        # Output size from the Autoencoder : [batch_size, 3, 32, 32]\n",
        "        self.encoder = nn.Sequential(\n",
        "            # 3 input channels(RGB), 12 output channels, 4x4 square convolution\n",
        "            nn.Conv2d(in_channels = 3, out_channels = 12, kernel_size = 4, stride=2, padding=1), \n",
        "            nn.ReLU(),# Output size : [batch_size, 12, 32, 32]\n",
        "            nn.Conv2d(in_channels = 12, out_channels = 24, kernel_size = 4, stride=2, padding=1), \n",
        "            nn.ReLU(),# Output size : [batch_size, 24, 32, 32]\n",
        "            nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size = 4, stride=2, padding=1),\n",
        "            nn.ReLU() \n",
        "            ## Output size : [batch_size, 48, 4, 4]\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          \n",
        "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1), \n",
        "            nn.ReLU(),  #TODO : try with LeakyReLU while decoding\n",
        "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), # Output size: [batch_size, 12, 32, 32]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1), # Output size: [batch_size, 3, 32, 32]\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqlgY0Yv75SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_AEmodel():\n",
        "    autoencoder = Autoencoder()\n",
        "    if torch.cuda.is_available():\n",
        "        autoencoder = autoencoder.cuda()\n",
        "        print(\"Model moved to GPU.\")\n",
        "    return autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urYXhK9675V6",
        "colab_type": "code",
        "outputId": "10a5c5c6-3193-40b1-bfbe-c251d631a36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create model\n",
        "autoencoder = create_AEmodel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model moved to GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9JpFt_975dO",
        "colab_type": "code",
        "outputId": "fb993f8d-3b48-499c-8740-455056c56430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load pretrained weights\n",
        "autoencoder.load_state_dict(torch.load('./autoencoder.pkl'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-y040lYnXvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ptb1GeXm5yk",
        "colab_type": "code",
        "outputId": "e74092f8-aa5c-491e-9de5-d9428cd85cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "summary(autoencoder, input_size=(3, 32, 32), batch_size=batch_size, device='cuda')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [16, 12, 16, 16]             588\n",
            "              ReLU-2           [16, 12, 16, 16]               0\n",
            "            Conv2d-3             [16, 24, 8, 8]           4,632\n",
            "              ReLU-4             [16, 24, 8, 8]               0\n",
            "            Conv2d-5             [16, 48, 4, 4]          18,480\n",
            "              ReLU-6             [16, 48, 4, 4]               0\n",
            "   ConvTranspose2d-7             [16, 24, 8, 8]          18,456\n",
            "              ReLU-8             [16, 24, 8, 8]               0\n",
            "   ConvTranspose2d-9           [16, 12, 16, 16]           4,620\n",
            "             ReLU-10           [16, 12, 16, 16]               0\n",
            "  ConvTranspose2d-11            [16, 3, 32, 32]             579\n",
            "          Sigmoid-12            [16, 3, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 47,355\n",
            "Trainable params: 0\n",
            "Non-trainable params: 47,355\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 3.19\n",
            "Params size (MB): 0.18\n",
            "Estimated Total Size (MB): 3.56\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyYoSqkRUJXf",
        "colab_type": "text"
      },
      "source": [
        "## Simple Classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxyguhvWUQXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=48, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(p=0.05)\n",
        "            \n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3KTQSUAOXe1",
        "colab_type": "text"
      },
      "source": [
        "## Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOp931LGoyLz",
        "colab_type": "code",
        "outputId": "7006dd25-a771-4481-dff2-9d240fb6b20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "#model = pretrainedmodel()\n",
        "model = CNNClassifier()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "summary(model, input_size=(48, 4, 4), batch_size=16, device='cuda')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1             [16, 48, 4, 4]          20,784\n",
            "       BatchNorm2d-2             [16, 48, 4, 4]              96\n",
            "              ReLU-3             [16, 48, 4, 4]               0\n",
            "            Conv2d-4             [16, 32, 4, 4]          13,856\n",
            "              ReLU-5             [16, 32, 4, 4]               0\n",
            "         MaxPool2d-6             [16, 32, 2, 2]               0\n",
            "            Conv2d-7             [16, 32, 2, 2]           9,248\n",
            "       BatchNorm2d-8             [16, 32, 2, 2]              64\n",
            "              ReLU-9             [16, 32, 2, 2]               0\n",
            "        Dropout2d-10             [16, 32, 2, 2]               0\n",
            "          Dropout-11                  [16, 128]               0\n",
            "           Linear-12                   [16, 64]           8,256\n",
            "             ReLU-13                   [16, 64]               0\n",
            "           Linear-14                   [16, 32]           2,080\n",
            "             ReLU-15                   [16, 32]               0\n",
            "           Linear-16                   [16, 10]             330\n",
            "================================================================\n",
            "Total params: 54,714\n",
            "Trainable params: 54,714\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 0.52\n",
            "Params size (MB): 0.21\n",
            "Estimated Total Size (MB): 0.78\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNGyDQlehmRO",
        "colab_type": "text"
      },
      "source": [
        "#### Mapping of Classes to Indexes\n",
        "\n",
        "To keep track of the predictions made by the model, we create a mapping of classes to indexes and indexes to classes. This will let us know the actual class for a given prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkdxDUFzhmRO",
        "colab_type": "code",
        "outputId": "3af3c4c2-c7ec-4908-91bd-8289bce17964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "model.class_to_idx = trainset.class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(model.idx_to_class.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'airplane'),\n",
              " (1, 'automobile'),\n",
              " (2, 'bird'),\n",
              " (3, 'cat'),\n",
              " (4, 'deer'),\n",
              " (5, 'dog'),\n",
              " (6, 'frog'),\n",
              " (7, 'horse'),\n",
              " (8, 'ship'),\n",
              " (9, 'truck')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-DZAGAMhmRQ",
        "colab_type": "text"
      },
      "source": [
        "# Training Loss and Optimizer\n",
        "\n",
        "The loss is the negative log likelihood and the optimizer is the Adam optimizer. The negative log likelihood in PyTorch expects log probabilities so we need to pass it the raw output from the log softmax in our model's final layer. The optimizer is told to optimizer the model parameters (only a few of which require a gradient). \n",
        "\n",
        "* Loss (criterion): keeps track of the loss itself and the gradients of the loss with respect to the model parameters (weights)\n",
        "* Optimizer: updates the parameters (weights) with the gradients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLaiag0hmRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() #nn.NLLLoss()\n",
        "# NLLLoss  is particularly useful when you have an unbalanced training set.\n",
        "# https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DZ6FZzhhmRT",
        "colab_type": "text"
      },
      "source": [
        "Below we can look at the parameters (weights) that will be updated by the optimizer during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpGQ9IHqhmRU",
        "colab_type": "code",
        "outputId": "e1637a05-73df-485c-8b46-85526ed10dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "for p in optimizer.param_groups[0]['params']:\n",
        "    if p.requires_grad:\n",
        "        print(p.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([48, 48, 3, 3])\n",
            "torch.Size([48])\n",
            "torch.Size([48])\n",
            "torch.Size([48])\n",
            "torch.Size([32, 48, 3, 3])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 32, 3, 3])\n",
            "torch.Size([32])\n",
            "torch.Size([32])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64])\n",
            "torch.Size([32, 64])\n",
            "torch.Size([32])\n",
            "torch.Size([10, 32])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmIjV9HFhmRW",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "For training, we iterate through the train `DataLoader`, each time passing one batch through the model. One complete pass through the training data is known as an `epoch`, and we train for a set number of epochs or until early stopping kicks in (more below). After each batch, we calculate the loss (with `criterion(output, targets)`) and then calculate the gradients of the loss with respect to the model parameters with `loss.backward()`. This uses autodifferentiation and backpropagation to calculate the gradients. \n",
        "\n",
        "After calculating the gradients, we call `optimizer.step()` to update the model parameters with the gradients. This is done on every training batch so we are implementing stochastic gradient descent (or rather a version of it with momentum known as Adam). For each batch, we also compute the accuracy for monitoring and after the training loop has completed, we start the validation loop. This will be used to carry out early stopping.\n",
        "\n",
        "\n",
        "## Early Stopping\n",
        "\n",
        "Early stopping halts the training when the validation loss has not decreased for a number of epochs. Each time the validation loss does decrease, the model weights are saved so we can later load in the best model. Early stopping is an effective method to prevent overfitting on the training data. If we continue training, the training loss will continue to decrease, but the validation loss will increase because the model is starting to memorize the training data. Early stopping prevents this from happening, and, if we save the model each epoch when the validation loss decreases, we are able to retrieve the model that does best on the validation data.\n",
        "\n",
        "Early stopping is implemented by iterating through the validation data at the end of each training epoch and calculating the loss. We use the complete validation data every time and record whether or not the loss has decreased. If it has not for a number of epochs, we stop training, retrieve the best weights, and return them. When in the validation loop, we make sure not to update the model parameters. \n",
        "\n",
        "### Training Function\n",
        "\n",
        "The below function trains the network while monitoring a number of different parameters. We train with early stopping on the validation set. There are a number of parameters that I've tried to explain in the doc string. Hopefully, the comments and background make things somewhat understandable! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm0fNIvchmRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,\n",
        "          autoencoder,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20,\n",
        "          print_every=2):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            \n",
        "            \n",
        "            # load autoencoder\n",
        "            encoded,_ = autoencoder(data)\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(encoded)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if torch.cuda.is_available():\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "                    encoded,_ = autoencoder(data)\n",
        "                    # Forward pass\n",
        "                    output = model(encoded)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccbl_DK4hmRZ",
        "colab_type": "code",
        "outputId": "08440c6a-8ea5-4fe7-c21f-0b3eacdef069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        }
      },
      "source": [
        "model, history = train(\n",
        "    model,\n",
        "    autoencoder,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name='ensemble.pkl',\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=30,\n",
        "    print_every=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training from Scratch.\n",
            "\n",
            "\n",
            "Epoch: 1 \tTraining Loss: 1.4869 \tValidation Loss: 1.4662\n",
            "\t\tTraining Accuracy: 47.10%\t Validation Accuracy: 46.38%\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 1.3508 \tValidation Loss: 1.4163\n",
            "\t\tTraining Accuracy: 51.75%\t Validation Accuracy: 48.58%\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 1.2860 \tValidation Loss: 1.4608\n",
            "\t\tTraining Accuracy: 54.04%\t Validation Accuracy: 48.76%\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 1.2407 \tValidation Loss: 1.4037\n",
            "\t\tTraining Accuracy: 56.00%\t Validation Accuracy: 50.58%\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 1.2074 \tValidation Loss: 1.3560\n",
            "\t\tTraining Accuracy: 57.27%\t Validation Accuracy: 51.76%\n",
            "\n",
            "Epoch: 11 \tTraining Loss: 1.1782 \tValidation Loss: 1.3262\n",
            "\t\tTraining Accuracy: 57.85%\t Validation Accuracy: 52.50%\n",
            "\n",
            "Epoch: 13 \tTraining Loss: 1.1540 \tValidation Loss: 1.2758\n",
            "\t\tTraining Accuracy: 58.99%\t Validation Accuracy: 53.52%\n",
            "\n",
            "Epoch: 15 \tTraining Loss: 1.1334 \tValidation Loss: 1.2668\n",
            "\t\tTraining Accuracy: 59.56%\t Validation Accuracy: 55.02%\n",
            "\n",
            "Epoch: 17 \tTraining Loss: 1.1172 \tValidation Loss: 1.3036\n",
            "\t\tTraining Accuracy: 60.32%\t Validation Accuracy: 53.66%\n",
            "\n",
            "Epoch: 19 \tTraining Loss: 1.0999 \tValidation Loss: 1.2788\n",
            "\t\tTraining Accuracy: 60.68%\t Validation Accuracy: 54.76%\n",
            "\n",
            "Epoch: 21 \tTraining Loss: 1.0868 \tValidation Loss: 1.2916\n",
            "\t\tTraining Accuracy: 61.47%\t Validation Accuracy: 54.66%\n",
            "\n",
            "Epoch: 23 \tTraining Loss: 1.0746 \tValidation Loss: 1.2246\n",
            "\t\tTraining Accuracy: 61.96%\t Validation Accuracy: 56.66%\n",
            "\n",
            "Epoch: 25 \tTraining Loss: 1.0623 \tValidation Loss: 1.2283\n",
            "\t\tTraining Accuracy: 62.43%\t Validation Accuracy: 56.18%\n",
            "\n",
            "Epoch: 27 \tTraining Loss: 1.0494 \tValidation Loss: 1.2098\n",
            "\t\tTraining Accuracy: 62.39%\t Validation Accuracy: 57.30%\n",
            "\n",
            "Epoch: 29 \tTraining Loss: 1.0459 \tValidation Loss: 1.2498\n",
            "\t\tTraining Accuracy: 62.84%\t Validation Accuracy: 56.32%\n",
            "\n",
            "Best epoch: 27 with loss: 1.21 and acc: 56.32%\n",
            "856.19 total seconds elapsed. 29.52 seconds per epoch.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB7_SMsuhmSN",
        "colab_type": "text"
      },
      "source": [
        "## (last)Function to Evaluate Model Over All Classes\n",
        "\n",
        "The next function iterates through the testing set in order to make predictions for each image. It calculates performance for each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG2VdfBvIy0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target, topk=(1, )):\n",
        "    \"\"\"Compute the topk accuracy(s)\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        output = output.to('cuda')\n",
        "        target = target.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # Find the predicted classes and transpose\n",
        "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
        "        pred = pred.t()\n",
        "\n",
        "        # Determine predictions equal to the targets\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "\n",
        "        # For each k, find the percentage of correct\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr4mmM4IhmSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, autoencoder,test_loader, criterion, topk=(1, 5)):\n",
        "    \"\"\"Measure the performance of a trained PyTorch model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn for inference\n",
        "        test_loader (PyTorch DataLoader): test dataloader\n",
        "        topk (tuple of ints): accuracy to measure\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        results (DataFrame): results for each category\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    classes = []\n",
        "    losses = []\n",
        "    # Hold accuracy results\n",
        "    acc_results = np.zeros((len(test_loader.dataset), len(topk)))\n",
        "    i = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Testing loop\n",
        "        for data, targets in test_loader:\n",
        "\n",
        "            # Tensors to gpu\n",
        "            if torch.cuda.is_available():\n",
        "                data, targets = data.to('cuda'), targets.to('cuda')\n",
        "\n",
        "            # Raw model output\n",
        "            encoded,_ = autoencoder(data)\n",
        "            out = model(encoded)\n",
        "            # Iterate through each example\n",
        "            for pred, true in zip(out, targets):\n",
        "                # Find topk accuracy\n",
        "                acc_results[i, :] = accuracy(\n",
        "                    pred.unsqueeze(0), true.unsqueeze(0), topk)\n",
        "                classes.append(model.idx_to_class[true.item()])\n",
        "                # Calculate the loss\n",
        "                loss = criterion(pred.view(1, n_classes), true.view(1))\n",
        "                losses.append(loss.item())\n",
        "                i += 1\n",
        "\n",
        "    # Send results to a dataframe and calculate average across classes\n",
        "    results = pd.DataFrame(acc_results, columns=[f'top{i}' for i in topk])\n",
        "    results['class'] = classes\n",
        "    results['loss'] = losses\n",
        "    results = results.groupby(classes).mean()\n",
        "\n",
        "    return results.reset_index().rename(columns={'index': 'class'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0H0Xjcx5_TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "# Evaluate the model on all the training data\n",
        "results = evaluate(model, autoencoder,dataloaders['test'], criterion)\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}